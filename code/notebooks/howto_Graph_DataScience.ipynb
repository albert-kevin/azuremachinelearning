{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc44493",
   "metadata": {},
   "source": [
    "Author: Kevin ALBERT  \n",
    "\n",
    "Created: May 2021 (Updated: 11 Aug 2021)\n",
    "\n",
    "TestRun: 12 Aug 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ebf00",
   "metadata": {},
   "source": [
    "# Graph Data Science Algorithms\n",
    "_**How to enrich data features using graph networks to improve machine learning model predictions using Neo4j**_  \n",
    "\n",
    "1. [UNDERSTAND](#Understand-Algorithms)  \n",
    "1. [DEMONSTRATE](#Demonstrate-Algorithms)  \n",
    "1. [SOURCES](#Source-Documentation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install python modules\n",
    "!conda install -y -c conda-forge py2neo pandas pyarrow fastparquet psutil azure-storage-blob\n",
    "!conda install -y -c conda-forge seaborn xlrd\n",
    "!pip install monotonic packaging scikit-plot raiwidgets azureml-core azureml-interpret interpret\n",
    "!pip install git+https://github.com/ybaktir/networkx-neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete database and restart (~2min)\n",
    "import os\n",
    "os.system(\" cd ../.. && \\\n",
    "            sudo docker-compose down && \\\n",
    "            sudo rm -Rf neo4j/data/databases/neo4j && \\\n",
    "            sudo rm -Rf neo4j/data/transactions/neo4j && \\\n",
    "            sudo docker-compose up --build &\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883ca623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# pd.describe_option('display')            # show all pandas options, parameters can slow down notebook\n",
    "pd.set_option('display.max_colwidth', 100) # default 50, the maximum width in characters of a column\n",
    "pd.set_option('display.max_columns', 40)   # default 20, the maximum amount of columns in view \n",
    "pd.set_option('display.max_rows', 60)      # default 60, the maximum amount of rows in view\n",
    "\n",
    "# Javascript\n",
    "from IPython.display import Javascript\n",
    "\n",
    "# data visualisation packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "\n",
    "# explainer packages\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.blackbox import PFIExplainer \n",
    "from interpret.ext.glassbox import LGBMExplainableModel\n",
    "from interpret.ext.glassbox import LinearExplainableModel\n",
    "from interpret.ext.glassbox import SGDExplainableModel\n",
    "from interpret.ext.glassbox import DecisionTreeExplainableModel\n",
    "\n",
    "# azureml packages\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.interpret import ExplanationClient\n",
    "from azureml.core import Workspace, Dataset, Datastore, Run\n",
    "\n",
    "# enable dashboard rendering on self-hosted JupyterHub\n",
    "from interpret import show, preserve\n",
    "from interpret.provider import InlineProvider\n",
    "from interpret import set_visualize_provider\n",
    "set_visualize_provider(InlineProvider())\n",
    "\n",
    "# interpretml packages\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "# interpret-community packages\n",
    "from raiwidgets import ExplanationDashboard\n",
    "from raiwidgets import ErrorAnalysisDashboard\n",
    "\n",
    "# machine learning packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# graph neo4j packages\n",
    "from py2neo import Graph, Node, Relationship\n",
    "import nxneo4j as nx\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# operating system packages\n",
    "import platform\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb4c059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores : 2 (3GHz)\n",
      "Memory: 7.78 GB (34.0%)\n",
      "Swap  : 8 GB\n",
      "Disk  : 145 GB (68.5% ext4)\n",
      "System: 18.04.1-Ubuntu\n"
     ]
    }
   ],
   "source": [
    "# environment\n",
    "print(f\"Cores : {psutil.cpu_count(logical=True)} ({psutil.cpu_freq().current/1000:.0f}GHz)\")\n",
    "print(f\"Memory: {psutil.virtual_memory().total/(1024**3):.2f} GB ({psutil.virtual_memory().percent}%)\")\n",
    "print(f\"Swap  : {os.path.getsize('/swapfile')/(1024**3):.0f} GB\")\n",
    "disk_size = psutil.disk_usage(psutil.disk_partitions()[0].mountpoint).total\n",
    "disk_used = psutil.disk_usage(psutil.disk_partitions()[0].mountpoint).percent\n",
    "disk_fs   = psutil.disk_partitions()[0].fstype \n",
    "print(f\"Disk  : {disk_size/(1024**3):.0f} GB ({disk_used}% {disk_fs})\")\n",
    "print(f\"System: {platform.uname().version.split('~')[1].split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9ae2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conda       : 4.10.1\n",
      "pip         : 21.1.2\n",
      "python      : 3.8.10\n",
      "pandas      : 1.2.4\n",
      "numpy       : 1.20.3\n",
      "matplotlib  : 3.4.2\n",
      "seaborn     : 0.11.1\n",
      "scikit-plot : 0.3.7\n",
      "sklearn     : 0.24.2\n",
      "py2neo      : 2021.1.1\n",
      "raiwidgets  : 0.4.0\n",
      "interpret   : 0.2.4\n",
      "azureml-core: 1.30.0\n",
      "nxneo4j     : 0.0.3\n"
     ]
    }
   ],
   "source": [
    "# installed python modules\n",
    "conda_version = ! conda -V\n",
    "print(f\"conda       : {conda_version[0].split()[1]}\")\n",
    "pip_version = ! pip -V\n",
    "print(f\"pip         : {pip_version[0].split()[1]}\")\n",
    "python_version = ! python -V\n",
    "print(f\"python      : {python_version[0].split()[1]}\")\n",
    "pandas_version = ! pip list |grep -i pandas\n",
    "print(f\"pandas      : {pandas_version[0].split()[1]}\")\n",
    "numpy_version = ! pip list |grep -ie \"^numpy \"\n",
    "print(f\"numpy       : {numpy_version[0].split()[1]}\")\n",
    "matplotlib_version = ! pip list |grep -ie \"^matplotlib \"\n",
    "print(f\"matplotlib  : {matplotlib_version[0].split()[1]}\")\n",
    "seaborn_version = ! pip list |grep -ie \"^seaborn \"\n",
    "print(f\"seaborn     : {seaborn_version[0].split()[1]}\")\n",
    "scikit_plot_version = ! pip list |grep -ie \"^scikit-plot \"\n",
    "print(f\"scikit-plot : {scikit_plot_version[0].split()[1]}\")\n",
    "sklearn_version = ! pip list |grep -ie \"^scikit-learn \"\n",
    "print(f\"sklearn     : {sklearn_version[0].split()[1]}\")\n",
    "py2neo_version = ! pip list |grep -i py2neo\n",
    "print(f\"py2neo      : {py2neo_version[0].split()[1]}\")\n",
    "raiwidgets_version = ! pip list |grep -ie \"^raiwidgets \"\n",
    "print(f\"raiwidgets  : {raiwidgets_version[0].split()[1]}\")\n",
    "interpret_version = ! pip list |grep -ie \"^interpret \"\n",
    "print(f\"interpret   : {interpret_version[0].split()[1]}\")\n",
    "azuremlcore_version = ! pip list |grep -ie \"^azureml-core \"\n",
    "print(f\"azureml-core: {azuremlcore_version[0].split()[1]}\")\n",
    "networkx_neo4j_version = ! pip list |grep -ie \"^networkx-neo4j \"\n",
    "print(f\"nxneo4j     : {networkx_neo4j_version[0].split()[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf286c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph connection instance\n",
    "server  = \"20.86.118.190\"\n",
    "port    = \"7687\"\n",
    "user    = \"neo4j\"\n",
    "passw   = \"digityser\"\n",
    "db_name = \"neo4j\"        # default name in v4.x\n",
    "uri     = \"bolt://\"+server+\":\"+port\n",
    "\n",
    "graph = Graph(host=server, auth=(user, passw), name=db_name) # py2neo instance\n",
    "\n",
    "driver = GraphDatabase.driver(uri=uri, auth=(user,passw))    # nxneo4j instance\n",
    "G = nx.DiGraph(driver) # directed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b691ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " name         | versions  | edition    \n",
       "--------------|-----------|------------\n",
       " Neo4j Kernel | ['4.2.6'] | enterprise "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neo4j database version\n",
    "graph.call.dbms.components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48689f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 94288\r\n",
      "-rw-r--r-- 1 ubuntu root 21208168 May  5 22:16 apoc-4.2.0.4-all.jar\r\n",
      "-rw-r--r-- 1 ubuntu root 13429472 May  5 22:17 apoc-couchbase-dependencies-4.2.0.4.jar\r\n",
      "-rw-r--r-- 1 ubuntu root   709134 May  5 22:17 apoc-email-dependencies-4.2.0.4.jar\r\n",
      "-rw-r--r-- 1 ubuntu root  1483696 May  5 22:17 apoc-mongodb-dependencies-4.2.0.4.jar\r\n",
      "-rw-r--r-- 1 ubuntu root 10848419 May  5 22:17 apoc-nlp-dependencies-4.2.0.4.jar\r\n",
      "-rw-r--r-- 1 ubuntu root 13956780 May  5 22:17 apoc-xls-dependencies-4.2.0.4.jar\r\n",
      "-rw-r--r-- 1 ubuntu root 11162611 Apr 28 16:06 bloom-plugin-4.x-1.6.1.jar\r\n",
      "-rw-r--r-- 1 ubuntu root       84 May 26 16:25 bloom-plugin.license\r\n",
      "-rw-r--r-- 1 ubuntu root  9229517 Apr 28 16:06 neo4j-bloom-1.6.1-assets.zip\r\n",
      "-rw-r--r-- 1 ubuntu root 14498309 May  6 07:43 neo4j-graph-data-science-1.5.2.jar\r\n"
     ]
    }
   ],
   "source": [
    "# neo4j plugins\n",
    "! sudo ls -l ../../neo4j/plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9726255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "dbms.default_listen_address=0.0.0.0\r\n",
      "\r\n",
      "\r\n",
      "neo4j.bloom.license_file=/plugins/bloom-plugin.license\r\n",
      "neo4j.bloom.authorization_role=admin,architect\r\n",
      "dbms.unmanaged_extension_classes=com.neo4j.bloom.server=/browser/bloom\r\n",
      "dbms.tx_log.rotation.retention_policy=100M size\r\n",
      "dbms.security.procedures.whitelist=apoc.*,gds.*\r\n",
      "dbms.security.procedures.unrestricted=apoc.*,gds.*,bloom.*\r\n",
      "dbms.memory.pagecache.size=2G\r\n",
      "dbms.memory.heap.max_size=2G\r\n",
      "dbms.directories.plugins=/plugins\r\n",
      "dbms.directories.logs=/logs\r\n",
      "dbms.directories.import=/import\r\n",
      "causal_clustering.transaction_advertised_address=1c3465c44514:6000\r\n",
      "causal_clustering.raft_advertised_address=1c3465c44514:7000\r\n",
      "causal_clustering.discovery_advertised_address=1c3465c44514:5000\r\n",
      "apoc.import.file.enabled=true\r\n"
     ]
    }
   ],
   "source": [
    "# neo4j database configuration\n",
    "! sudo cat ../../neo4j/conf/neo4j.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42459eec",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08598d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.world/nrippner/titanic-disaster-dataset\n",
    "df = pd.read_csv('https://query.data.world/s/d5rym7e5oz4zztlmwq6motdu2rcy2t')\n",
    "\n",
    "\n",
    "# drop all columns with all values NA\n",
    "df = df.dropna(axis=1, how='all')\n",
    "# drop all records with all values NA\n",
    "df = df.dropna(axis=0, how='all')\n",
    "# remove similar rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['passengerId'] = df.index\n",
    "\n",
    "# Calculate total size of family (same surname) including passenger\n",
    "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
    "\n",
    "# Add family_name column to easily identify relatives\n",
    "df['family_name'] = df['name'].str.split(',', expand=True)[0]\n",
    "\n",
    "# Fill NaN values for Cabins\n",
    "df['cabin'] = df['cabin'].fillna('No Cabin')\n",
    "\n",
    "# Extract deck from cabin number\n",
    "df['deck'] = df['cabin'].str[:1]\n",
    "\n",
    "# Fill NaN values for deck\n",
    "df['deck'] = df['deck'].fillna('No Deck')\n",
    "\n",
    "# Fill incorrect NaN values of embarked with correct values for passengers\n",
    "df.loc[df['ticket'] == '113572', 'embarked'] = 'S'\n",
    "\n",
    "# Replace embarked with location name\n",
    "embarked = {\"S\": \"Southampton\", \"C\": \"Cherbourg\", \"Q\": \"Queenstown\"}\n",
    "df['embarked'] = df['embarked'].map(embarked)\n",
    "\n",
    "# Replace NaN values with Unknown in destination column\n",
    "df['home.dest'] = df['home.dest'].fillna(\"No Home\")\n",
    "\n",
    "# replace nan with median value\n",
    "df['age'] = df['age'].replace({np.nan: getattr(df['age'], 'median')()})\n",
    "df['fare'] = df['fare'].replace({np.nan: getattr(df['fare'], 'median')()})\n",
    "\n",
    "# remove irrelevant columns\n",
    "df = df.drop(['boat', 'body', 'home.dest'], axis=1)\n",
    "\n",
    "# replace feature names with interpretable naming (others are left as-is) \n",
    "df = df.rename(columns={'pclass':'pclass', 'survived':'survived', 'name':'fullname', 'sex':'sex', 'age':'age',\n",
    "                        'sibsp':'nrSiblings', 'parch':'nrParents', 'ticket':'ticketId',\n",
    "                        'fare':'ticketPrice', 'cabin':'cabin', 'embarked':'port', 'passenger':'passengerId',\n",
    "                        'family_size':'familySize','family_name':'familyName', 'deck':'deck'})\n",
    "\n",
    "# keep only numeric ID values\n",
    "df['ticketId'] = df['ticketId'].apply(lambda x: ''.join(c for c in x if c.isnumeric()))\n",
    "\n",
    "# Replace NaN values with Unknown in destination column\n",
    "df['ticketId'] = df['ticketId'].replace('', '0')\n",
    "\n",
    "# Save final dataset to data/silver\n",
    "df.to_csv(\"../../data/silver/titanicgraph/titanic.csv\", sep=',', index=False)\n",
    "# Save final dataset to neo4j/import\n",
    "df.to_csv(\"../../neo4j/import/titanic.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned dataset\n",
    "df = pd.read_csv('../../data/silver/titanicgraph/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d49e51",
   "metadata": {},
   "source": [
    "## Passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bcd4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neo4j/import dataset \"filename\"\n",
    "file = \"titanic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d7ece38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels_added': 1309, 'nodes_created': 1309, 'properties_set': 18326}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.run(\"CREATE CONSTRAINT ON (p:Passenger) ASSERT p.passengerId IS UNIQUE\")\n",
    "\n",
    "query = \"\"\"\n",
    "USING PERIODIC COMMIT 1000\n",
    "LOAD CSV WITH HEADERS FROM 'file:///\"\"\"+file+\"\"\"' AS line FIELDTERMINATOR ','\n",
    "MERGE (p:Passenger {passengerId:toInteger(line.passengerId),\n",
    "                    survived:toInteger(line.survived),\n",
    "                    fullname:(line.fullname),\n",
    "                    sex:(line.sex),\n",
    "                    age:toInteger(line.age),\n",
    "                    nrSiblings:toInteger(line.nrSiblings),\n",
    "                    nrParents:toInteger(line.nrParents),\n",
    "                    familySize:toInteger(line.familySize),\n",
    "                    familyName:(line.familyName),\n",
    "                    port:toString(line.port),\n",
    "                    pclass:toInteger(line.pclass),\n",
    "                    ticketId:(line.ticketId),\n",
    "                    cabinId:(line.cabin),\n",
    "                    ticketPrice:toFloat(line.ticketPrice)})\n",
    "\"\"\"\n",
    "graph.run(query).stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d341caef",
   "metadata": {},
   "source": [
    "## Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8cef89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels_added': 187, 'nodes_created': 187, 'properties_set': 187}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'relationships_created': 1309}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'nodes_deleted': 1, 'relationships_deleted': 1014}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph.run(\"CREATE CONSTRAINT ON (c:Cabin) ASSERT c.cabin IS UNIQUE\")\n",
    "\n",
    "query = \"\"\"\n",
    "USING PERIODIC COMMIT 1000\n",
    "LOAD CSV WITH HEADERS FROM 'file:///\"\"\"+file+\"\"\"' AS line FIELDTERMINATOR ','\n",
    "MERGE (c:Cabin {cabin:(line.cabin)})\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())\n",
    "\n",
    "query = \"\"\"\n",
    "USING PERIODIC COMMIT 1000\n",
    "LOAD CSV WITH HEADERS FROM 'file:///\"\"\"+file+\"\"\"' AS line FIELDTERMINATOR ','\n",
    "MATCH (p:Passenger {passengerId:toInteger(line.passengerId)})\n",
    "MATCH (c:Cabin {cabin:(line.cabin)})\n",
    "MERGE (p)-[:SAME_CABIN]->(c)\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (c:Cabin {cabin:'No Cabin'})\n",
    "DETACH DELETE c\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bbddb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.7 ms, sys: 352 µs, total: 3.05 ms\n",
      "Wall time: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "CALL gds.graph.create('myCabinGraph',\n",
    "    ['Passenger', 'Cabin'],\n",
    "    {\n",
    "        SLEPT_IN: {\n",
    "            type: 'SAME_CABIN',\n",
    "            orientation: 'NATURAL'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5020d44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.83 ms, sys: 368 µs, total: 3.19 ms\n",
      "Wall time: 743 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "CALL gds.nodeSimilarity.mutate('myCabinGraph', {\n",
    "    similarityCutoff: 0.00001,\n",
    "    degreeCutoff: 1,\n",
    "    topK: 10000,\n",
    "    mutateProperty: 'score',\n",
    "    mutateRelationshipType: 'SAME_CABIN'\n",
    "})\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d5d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.54 ms, sys: 747 µs, total: 9.29 ms\n",
      "Wall time: 35.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "CALL gds.alpha.degree.write('myCabinGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['SAME_CABIN'],\n",
    "    writeProperty: 'degree_cabin'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.triangleCount.write('myCabinGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['SAME_CABIN'],\n",
    "    writeProperty: 'trianglesCount_cabin'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.localClusteringCoefficient.write('myCabinGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['SAME_CABIN'],\n",
    "    writeProperty: 'clusteringCoefficient_cabin'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.wcc.write('myCabinGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['SAME_CABIN'],\n",
    "    writeProperty: 'weaklyconnectedcomponents_cabin'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.betweenness.write('myCabinGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['SAME_CABIN'],\n",
    "    writeProperty: 'betweenness_cabin'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.pageRank.write('myCabinGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['SAME_CABIN'],\n",
    "    dampingFactor: 0.85,\n",
    "    tolerance: 0.0000001,\n",
    "    maxIterations: 20,\n",
    "    writeProperty: 'pagerank_cabin'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.louvain.write('myCabinGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['SAME_CABIN'],\n",
    "    includeIntermediateCommunities: false,\n",
    "    writeProperty: 'louvain_cabin'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.fastRP.write('myCabinGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['SAME_CABIN'],\n",
    "    embeddingDimension: 4,\n",
    "    writeProperty:'fastrp_cabin'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.alpha.node2vec.write('myCabinGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['SAME_CABIN'],\n",
    "    embeddingDimension: 4,\n",
    "    iterations: 10,\n",
    "    walkLength: 80,\n",
    "    inOutFactor: 1,\n",
    "    returnFactor: 1,\n",
    "    relationshipWeightProperty: null,\n",
    "    writeProperty: 'node2vec_cabin'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62bca3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(No data)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all similarity relationships\n",
    "graph.run(\"\"\"MATCH p=()-[r:SAME_CABIN]->() DETACH DELETE r\"\"\")\n",
    "\n",
    "# remove all cabin nodes\n",
    "graph.run(\"\"\"MATCH (c:Cabin) DETACH DELETE c\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a3866",
   "metadata": {},
   "source": [
    "## TicketId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a462fe84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels_added': 924, 'nodes_created': 924, 'properties_set': 924}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'relationships_created': 1309}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'nodes_deleted': 1, 'relationships_deleted': 4}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph.run(\"CREATE CONSTRAINT ON (t:Ticket) ASSERT t.ticketId IS UNIQUE\")\n",
    "\n",
    "query = \"\"\"\n",
    "USING PERIODIC COMMIT 1000\n",
    "LOAD CSV WITH HEADERS FROM 'file:///\"\"\"+file+\"\"\"' AS line FIELDTERMINATOR ','\n",
    "MERGE (t:Ticket {ticketId:(line.ticketId)})\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())\n",
    "\n",
    "query = \"\"\"\n",
    "USING PERIODIC COMMIT 1000\n",
    "LOAD CSV WITH HEADERS FROM 'file:///\"\"\"+file+\"\"\"' AS line FIELDTERMINATOR ','\n",
    "MATCH (p:Passenger {passengerId:toInteger(line.passengerId)})\n",
    "MATCH (t:Ticket {ticketId:(line.ticketId)})\n",
    "MERGE (p)-[:HELD_TICKET]->(t)\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (t:Ticket {ticketId:'0'})\n",
    "DETACH DELETE t\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "270c9741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.63 ms, sys: 0 ns, total: 2.63 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "CALL gds.graph.create('myTicketGraph',\n",
    "    ['Passenger', 'Ticket'],\n",
    "    {\n",
    "        HOLD_ON: {\n",
    "            type: 'HELD_TICKET',\n",
    "            orientation: 'NATURAL'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f581f43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.78 ms, sys: 0 ns, total: 2.78 ms\n",
      "Wall time: 397 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "CALL gds.nodeSimilarity.mutate('myTicketGraph', {\n",
    "    similarityCutoff: 0.00001,\n",
    "    degreeCutoff: 1,\n",
    "    topK: 10000,\n",
    "    mutateProperty: 'score',\n",
    "    mutateRelationshipType: 'HELD_TICKET'\n",
    "})\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4813538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.55 ms, sys: 0 ns, total: 9.55 ms\n",
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "CALL gds.alpha.degree.write('myTicketGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['HELD_TICKET'],\n",
    "    writeProperty: 'degree_ticket'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.triangleCount.write('myTicketGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['HELD_TICKET'],\n",
    "    writeProperty: 'trianglesCount_ticket'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.localClusteringCoefficient.write('myTicketGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['HELD_TICKET'],\n",
    "    writeProperty: 'clusteringCoefficient_ticket'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.wcc.write('myTicketGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['HELD_TICKET'],\n",
    "    writeProperty: 'weaklyconnectedcomponents_ticket'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.betweenness.write('myTicketGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['HELD_TICKET'],\n",
    "    writeProperty: 'betweenness_ticket'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.pageRank.write('myTicketGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['HELD_TICKET'],\n",
    "    dampingFactor: 0.85,\n",
    "    tolerance: 0.0000001,\n",
    "    maxIterations: 20,\n",
    "    writeProperty: 'pagerank_ticket'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.louvain.write('myTicketGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['HELD_TICKET'],\n",
    "    includeIntermediateCommunities: false,\n",
    "    writeProperty: 'louvain_ticket'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.fastRP.write('myTicketGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['HELD_TICKET'],\n",
    "    embeddingDimension: 4,\n",
    "    writeProperty:'fastrp_ticket'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.alpha.node2vec.write('myTicketGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['HELD_TICKET'],\n",
    "    embeddingDimension: 4,\n",
    "    iterations: 10,\n",
    "    walkLength: 80,\n",
    "    inOutFactor: 1,\n",
    "    returnFactor: 1,\n",
    "    relationshipWeightProperty: null,\n",
    "    writeProperty: 'node2vec_ticket'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f57e351b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(No data)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all similarity relationships\n",
    "graph.run(\"\"\"MATCH p=()-[r:HELD_TICKET]->() DETACH DELETE r\"\"\")\n",
    "\n",
    "# remove all ticket nodes\n",
    "graph.run(\"\"\"MATCH (t:Ticket) DETACH DELETE t\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe766f",
   "metadata": {},
   "source": [
    "## Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "307b923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# family = pd.read_csv('https://raw.githubusercontent.com/ybaktir/neo4ds/master/titanic/data/relationships.csv')\n",
    "# fixed some \"quote typos\":\n",
    "family = pd.read_csv('../../data/silver/titanicgraph/titanicrelationships.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "724de994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.34 s, sys: 78.2 ms, total: 3.42 s\n",
      "Wall time: 57.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# RELATIONSHIPS\n",
    "G.identifier_property = 'fullname'\n",
    "G.node_label = 'Passenger'\n",
    "\n",
    "for i in range(len(family)):\n",
    "    RELATIONSHIP = str(family.loc[i]['type'])\n",
    "    if RELATIONSHIP == 'nan':\n",
    "        G.relationship_type = 'CONNECTED'\n",
    "    else:\n",
    "        G.relationship_type = RELATIONSHIP\n",
    "    G.add_edge(str(family.loc[i]['u_name']),str(family.loc[i]['v_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6dc39e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.9 ms, sys: 0 ns, total: 2.9 ms\n",
      "Wall time: 132 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "CALL gds.graph.create('myFamilyGraph',\n",
    "    ['Passenger'],\n",
    "    {\n",
    "        R: {\n",
    "            type: '*',\n",
    "            orientation: 'UNDIRECTED'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\"\"\"\n",
    "display(graph.run(query).stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b89bffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 ms, sys: 0 ns, total: 10.8 ms\n",
      "Wall time: 1min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "CALL gds.alpha.degree.write('myFamilyGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['*'],\n",
    "    writeProperty: 'degree_family'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.triangleCount.write('myFamilyGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['*'],\n",
    "    writeProperty: 'trianglesCount_family'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.localClusteringCoefficient.write('myFamilyGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['*'],\n",
    "    writeProperty: 'clusteringCoefficient_family'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.wcc.write('myFamilyGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['*'],\n",
    "    writeProperty: 'weaklyconnectedcomponents_family'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.betweenness.write('myFamilyGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['*'],\n",
    "    writeProperty: 'betweenness_family'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.pageRank.write('myFamilyGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['*'],\n",
    "    dampingFactor: 0.85,\n",
    "    tolerance: 0.0000001,\n",
    "    maxIterations: 20,\n",
    "    writeProperty: 'pagerank_family'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.louvain.write('myFamilyGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['*'],\n",
    "    includeIntermediateCommunities: false,\n",
    "    writeProperty: 'louvain_family'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.fastRP.write('myFamilyGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['*'],\n",
    "    embeddingDimension: 4,\n",
    "    writeProperty:'fastrp_family'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.alpha.node2vec.write('myFamilyGraph', {\n",
    "    nodeLabels: ['Passenger'],\n",
    "    relationshipTypes: ['*'],\n",
    "    embeddingDimension: 4,\n",
    "    iterations: 10,\n",
    "    walkLength: 80,\n",
    "    inOutFactor: 1,\n",
    "    returnFactor: 1,\n",
    "    relationshipWeightProperty: null,\n",
    "    writeProperty: 'node2vec_family'})\n",
    "\"\"\"\n",
    "graph.run(query).stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415ef73",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9f7d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_pipeline(df):\n",
    "    # split features + target variable \n",
    "    X = df.drop('Survived', axis='columns')\n",
    "    y = df[\"Survived\"]\n",
    "\n",
    "    # store the dummy columns for each categorical feature\n",
    "    categorical = [col for col, value in X.iteritems() if value.dtype == 'object']\n",
    "    # store the numerical columns for each numerical feature\n",
    "    numerical = list(X.columns.difference(categorical))\n",
    "\n",
    "    # split data into (train + validation) and test\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101, stratify=y)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=100, shuffle=True)\n",
    "\n",
    "    # create the preprocessing pipelines for both numeric and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    # print(numeric_transformer)\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    # print(categorical_transformer)\n",
    "\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical),\n",
    "            ('cat', categorical_transformer, categorical)])\n",
    "    # print(transformations)\n",
    "\n",
    "    # append classifier to preprocessing pipeline (select best model and hyperparameters)\n",
    "    pipeline = Pipeline(steps=[('preprocessor', transformations),\n",
    "                               ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "    # from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    # from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "    # cv = KFold(n_splits=5, shuffle=True, random_state=12)  # !! IMPORTANT set between True/False + random_state for comparing\n",
    "    # cv = RepeatedKFold(n_splits=5, n_repeats=20, random_state=12)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=25, random_state=11)\n",
    "\n",
    "    # this is the auc score on the TRAINING data (uses training + validation)\n",
    "    scores = cross_val_score(pipeline, X, y, cv=cv, scoring='roc_auc')\n",
    "    # calculate 10 auc_scores and average result\n",
    "    # scores\n",
    "    print(\"mean   : {:2.1%}\\nmedian : {:2.1%}\".format(np.mean(scores), np.median(scores)))\n",
    "\n",
    "    # model = pipeline.fit(x_train, y_train)\n",
    "    model = pipeline.fit(X, y)\n",
    "#     return model\n",
    "\n",
    "    mimic_explainer = MimicExplainer(model=model.steps[-1][1],              # trained model (ex: LogisticRegression())\n",
    "                                     initialization_examples=X, \n",
    "                                     explainable_model=LGBMExplainableModel,   # glassbox model\n",
    "                                     augment_data=True, \n",
    "                                     max_num_of_augmentations=10, \n",
    "                                     features=X.columns,                       # ex: ['Pclass', 'Sex', 'Embarked']\n",
    "                                     classes=['not_survived', 'survived'],     # '1' = Survived\n",
    "                                     transformations=transformations)  \n",
    "\n",
    "    global_mimic_explanation = mimic_explainer.explain_global(X)\n",
    "    print(global_mimic_explanation.get_feature_importance_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb26fb5",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02068a92",
   "metadata": {},
   "source": [
    "### nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1df252cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean   : 84.5%\n",
      "median : 84.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "divide by zero encountered in true_divide\n",
      "divide by zero encountered in log\n",
      "categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sex': 1.9169174738302364, 'Pclass': 0.8424180504263347, 'TicketPrice': 0.5725878562394551, 'Age': 0.5053594556216413, 'Embarked': 0.2865208048814677, 'NrSiblings': 0.12506287555167436, 'NrParents': 0.08718427448102094}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "MATCH (p:Passenger)\n",
    "RETURN p.sex AS Sex,\n",
    "       p.age AS Age,\n",
    "       p.nrSiblings AS NrSiblings,\n",
    "       p.nrParents AS NrParents,\n",
    "       p.ticketPrice AS TicketPrice,\n",
    "       p.survived AS Survived,\n",
    "       p.pclass AS Pclass,\n",
    "       p.port AS Embarked\n",
    "\"\"\"\n",
    "df = graph.run(query).to_data_frame()\n",
    "\n",
    "df = df.astype({'Survived':'int', 'Sex':'str', 'Embarked':'str', 'Pclass':'str'})\n",
    "\n",
    "ml_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199d1fb",
   "metadata": {},
   "source": [
    "### cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99d4ee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean   : 84.8%\n",
      "median : 84.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "divide by zero encountered in true_divide\n",
      "divide by zero encountered in log\n",
      "categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sex': 1.8511358812187682, 'Pclass': 0.7035332616740584, 'TicketPrice': 0.5304023686263541, 'Age': 0.46645042971780576, 'Cabin': 0.36863472665305164, 'Embarked': 0.25312864125176016, 'NrSiblings': 0.152023621940328, 'NrParents': 0.14232638714017226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "MATCH (p:Passenger)\n",
    "RETURN p.sex AS Sex,\n",
    "       p.age AS Age,\n",
    "       p.nrSiblings AS NrSiblings,\n",
    "       p.nrParents AS NrParents,\n",
    "       p.ticketPrice AS TicketPrice,\n",
    "       p.survived AS Survived,\n",
    "       p.pclass AS Pclass,\n",
    "       p.port AS Embarked,\n",
    "       p.cabinId AS Cabin\n",
    "\"\"\"\n",
    "df = graph.run(query).to_data_frame()\n",
    "\n",
    "df = df.astype({'Survived':'int', 'Sex':'str', 'Embarked':'str', 'Pclass':'str'})\n",
    "df = df.astype({'Cabin':'str'})\n",
    "\n",
    "ml_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a353973",
   "metadata": {},
   "source": [
    "### cabin graphy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cbc3a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean   : 85.4%\n",
      "median : 85.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "divide by zero encountered in true_divide\n",
      "divide by zero encountered in log\n",
      "categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sex': 1.6013906418217314, 'louvain_cabin': 0.6620582967382125, 'wcc_cabin': 0.31138969434156, 'Age': 0.24893390225544446, 'node2vec_cabin2': 0.22302918633325158, 'node2vec_cabin4': 0.20628191915549066, 'TicketPrice': 0.1944638929700958, 'Embarked': 0.15457065760423086, 'node2vec_cabin3': 0.150366373401217, 'node2vec_cabin1': 0.11012944305426647, 'NrSiblings': 0.09991533133680242, 'pagerank_cabin': 0.08433893500068965, 'NrParents': 0.05807174696653961, 'fastrp_cabin2': 0.05278415779338961, 'fastrp_cabin3': 0.017880722187921444, 'fastrp_cabin4': 0.016040447093941312, 'Pclass': 0.010827995253345532, 'fastrp_cabin1': 0.0050386820051153865, 'degree_cabin': 0.004368819185353827, 'trianglesCount_cabin': 0.0, 'clusteringCoefficient_cabin': 0.0, 'betweenness_cabin': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "MATCH (p:Passenger)\n",
    "RETURN p.sex AS Sex,\n",
    "       p.age AS Age,\n",
    "       p.nrSiblings AS NrSiblings,\n",
    "       p.nrParents AS NrParents,\n",
    "       p.ticketPrice AS TicketPrice,\n",
    "       p.survived AS Survived,\n",
    "       p.pclass AS Pclass,\n",
    "       p.port AS Embarked,\n",
    "       \n",
    "       p.trianglesCount_cabin AS trianglesCount_cabin,\n",
    "       p.clusteringCoefficient_cabin AS clusteringCoefficient_cabin,\n",
    "       p.betweenness_cabin AS betweenness_cabin,\n",
    "       p.degree_cabin AS degree_cabin,\n",
    "       p.weaklyconnectedcomponents_cabin AS wcc_cabin,\n",
    "       p.louvain_cabin AS louvain_cabin,\n",
    "       p.pagerank_cabin AS pagerank_cabin,\n",
    "       p.node2vec_cabin AS node2vec_cabin,\n",
    "       p.fastrp_cabin AS fastrp_cabin\n",
    "\"\"\"\n",
    "df = graph.run(query).to_data_frame()\n",
    "df = df.astype({'Survived':'int', 'Sex':'str', 'Embarked':'str', 'Pclass':'str'})\n",
    "df = df.astype({'trianglesCount_cabin':'int',\n",
    "                'clusteringCoefficient_cabin':'object',\n",
    "                'betweenness_cabin':'object',\n",
    "                'degree_cabin':'int',\n",
    "                'wcc_cabin':'int',\n",
    "                'pagerank_cabin':'float',\n",
    "                'louvain_cabin':'int'})\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['node2vec_cabin'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('node2vec_cabin'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['node2vec_cabin'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['node2vec_cabin'], axis=1)\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['fastrp_cabin'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('fastrp_cabin'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['fastrp_cabin'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['fastrp_cabin'], axis=1)\n",
    "\n",
    "ml_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b9115",
   "metadata": {},
   "source": [
    "### passengerId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5553266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean   : 85.2%\n",
      "median : 85.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "divide by zero encountered in true_divide\n",
      "divide by zero encountered in log\n",
      "categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sex': 1.7971078955264312, 'PassengerId': 1.0116676771873503, 'Age': 0.4085999074557338, 'TicketPrice': 0.39135708940006125, 'Embarked': 0.25586586434448255, 'NrSiblings': 0.15499009479908615, 'NrParents': 0.06330438668632773, 'Pclass': 0.007474404541754354}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "MATCH (p:Passenger)\n",
    "RETURN p.sex AS Sex,\n",
    "       p.age AS Age,\n",
    "       p.nrSiblings AS NrSiblings,\n",
    "       p.nrParents AS NrParents,\n",
    "       p.ticketPrice AS TicketPrice,\n",
    "       p.survived AS Survived,\n",
    "       p.pclass AS Pclass,\n",
    "       p.port AS Embarked,\n",
    "       p.passengerId AS PassengerId\n",
    "\"\"\"\n",
    "df = graph.run(query).to_data_frame()\n",
    "\n",
    "df = df.astype({'Survived':'int', 'Sex':'str', 'Embarked':'str', 'Pclass':'str'})\n",
    "df = df.astype({'PassengerId':'int'})\n",
    "\n",
    "ml_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f90490",
   "metadata": {},
   "source": [
    "### ticketId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a25a00fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean   : 85.9%\n",
      "median : 85.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "divide by zero encountered in true_divide\n",
      "divide by zero encountered in log\n",
      "categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sex': 1.8668677894022927, 'Pclass': 0.8894699299894134, 'TicketId': 0.46913801003380295, 'Age': 0.4283001240353304, 'TicketPrice': 0.4147843900956752, 'Embarked': 0.16173419794883986, 'NrSiblings': 0.1410678327955804, 'NrParents': 0.05947214568017308}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "MATCH (p:Passenger)\n",
    "RETURN p.sex AS Sex,\n",
    "       p.age AS Age,\n",
    "       p.nrSiblings AS NrSiblings,\n",
    "       p.nrParents AS NrParents,\n",
    "       p.ticketPrice AS TicketPrice,\n",
    "       p.survived AS Survived,\n",
    "       p.pclass AS Pclass,\n",
    "       p.port AS Embarked,\n",
    "       p.ticketId AS TicketId\n",
    "\"\"\"\n",
    "df = graph.run(query).to_data_frame()\n",
    "\n",
    "df = df.astype({'Survived':'int', 'Sex':'str', 'Embarked':'str', 'Pclass':'str'})\n",
    "df = df.astype({'TicketId':'int'})\n",
    "\n",
    "ml_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed3d82",
   "metadata": {},
   "source": [
    "### ticketId graphy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45996fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean   : 87.8%\n",
      "median : 87.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "divide by zero encountered in true_divide\n",
      "divide by zero encountered in log\n",
      "categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sex': 1.6457364366782608, 'louvain_ticket': 0.7850741916600213, 'node2vec_ticket2': 0.25118025339740785, 'Age': 0.24468497581379503, 'TicketPrice': 0.20239069489463504, 'fastrp_ticket2': 0.17452394406825314, 'node2vec_ticket3': 0.17272376044912754, 'node2vec_ticket1': 0.16192841327901494, 'node2vec_ticket4': 0.1539259923210886, 'degree_ticket': 0.15057952291571094, 'wcc_ticket': 0.14995024897890605, 'Embarked': 0.11178066873697431, 'NrSiblings': 0.05469522435490206, 'fastrp_ticket1': 0.043447598360805334, 'fastrp_ticket4': 0.03755625059792137, 'pagerank_ticket': 0.03481824821858193, 'NrParents': 0.033015628477077875, 'fastrp_ticket3': 0.032742065314221386, 'trianglesCount_ticket': 0.027381730707593876, 'Pclass': 0.003632838485830826, 'betweenness_ticket': 0.0, 'clusteringCoefficient_ticket': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "MATCH (p:Passenger)\n",
    "RETURN p.sex AS Sex,\n",
    "       p.age AS Age,\n",
    "       p.nrSiblings AS NrSiblings,\n",
    "       p.nrParents AS NrParents,\n",
    "       p.ticketPrice AS TicketPrice,\n",
    "       p.survived AS Survived,\n",
    "       p.pclass AS Pclass,\n",
    "       p.port AS Embarked,\n",
    "       \n",
    "       p.trianglesCount_ticket AS trianglesCount_ticket,\n",
    "       p.clusteringCoefficient_ticket AS clusteringCoefficient_ticket,\n",
    "       p.betweenness_ticket AS betweenness_ticket,\n",
    "       p.degree_ticket AS degree_ticket,\n",
    "       p.weaklyconnectedcomponents_ticket AS wcc_ticket,\n",
    "       p.louvain_ticket AS louvain_ticket,\n",
    "       p.pagerank_ticket AS pagerank_ticket,\n",
    "       p.node2vec_ticket AS node2vec_ticket,\n",
    "       p.fastrp_ticket AS fastrp_ticket\n",
    "\"\"\"\n",
    "df = graph.run(query).to_data_frame()\n",
    "df = df.astype({'Survived':'int', 'Sex':'str', 'Embarked':'str', 'Pclass':'str'})\n",
    "df = df.astype({'trianglesCount_ticket':'int',\n",
    "                'clusteringCoefficient_ticket':'object',\n",
    "                'betweenness_ticket':'object',\n",
    "                'degree_ticket':'int',\n",
    "                'wcc_ticket':'int',\n",
    "                'pagerank_ticket':'float',\n",
    "                'louvain_ticket':'int'})\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['node2vec_ticket'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('node2vec_ticket'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['node2vec_ticket'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['node2vec_ticket'], axis=1)\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['fastrp_ticket'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('fastrp_ticket'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['fastrp_ticket'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['fastrp_ticket'], axis=1)\n",
    "\n",
    "ml_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b806db4f",
   "metadata": {},
   "source": [
    "### family graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "785f2e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean   : 86.8%\n",
      "median : 86.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "divide by zero encountered in true_divide\n",
      "divide by zero encountered in log\n",
      "categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sex': 1.5156115743128287, 'Pclass': 0.57576182121642, 'louvain_family': 0.24919321566000802, 'Age': 0.2362573010675135, 'node2vec_family1': 0.20969972414447124, 'TicketPrice': 0.16815105825082258, 'Embarked': 0.16272852873701726, 'node2vec_family4': 0.14221825116391199, 'wcc_family': 0.13864632982746178, 'node2vec_family3': 0.12269939204738986, 'node2vec_family2': 0.11171886026429037, 'degree_family': 0.08484397731355833, 'fastrp_family2': 0.08182241039138195, 'fastrp_family1': 0.07528459445522521, 'fastrp_family4': 0.07015141057532945, 'fastrp_family3': 0.06348500033830333, 'trianglesCount_family': 0.06153589315061519, 'NrSiblings': 0.05761960254156879, 'pagerank_family': 0.04927931745479048, 'clusteringCoefficient_family': 0.04569363522822957, 'NrParents': 0.03409140569377808, 'betweenness_family': 0.0007758760756744551}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "MATCH (p:Passenger)\n",
    "RETURN p.sex AS Sex,\n",
    "       p.age AS Age,\n",
    "       p.nrSiblings AS NrSiblings,\n",
    "       p.nrParents AS NrParents,\n",
    "       p.ticketPrice AS TicketPrice,\n",
    "       p.survived AS Survived,\n",
    "       p.pclass AS Pclass,\n",
    "       p.port AS Embarked,\n",
    "\n",
    "       p.trianglesCount_family AS trianglesCount_family,\n",
    "       p.clusteringCoefficient_family AS clusteringCoefficient_family,\n",
    "       p.betweenness_family AS betweenness_family,\n",
    "       p.degree_family AS degree_family,\n",
    "       p.weaklyconnectedcomponents_family AS wcc_family,\n",
    "       p.louvain_family AS louvain_family,\n",
    "       p.pagerank_family AS pagerank_family,\n",
    "       p.node2vec_family AS node2vec_family,\n",
    "       p.fastrp_family AS fastrp_family\n",
    "\"\"\"\n",
    "df = graph.run(query).to_data_frame()\n",
    "df = df.astype({'Survived':'int', 'Sex':'str', 'Embarked':'str', 'Pclass':'str'})\n",
    "df = df.astype({'trianglesCount_family':'int',\n",
    "                'clusteringCoefficient_family':'object',\n",
    "                'betweenness_family':'object',\n",
    "                'degree_family':'int',\n",
    "                'wcc_family':'int',\n",
    "                'pagerank_family':'float',\n",
    "                'louvain_family':'int'})\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['node2vec_family'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('node2vec_family'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['node2vec_family'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['node2vec_family'], axis=1)\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['fastrp_family'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('fastrp_family'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['fastrp_family'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['fastrp_family'], axis=1)\n",
    "\n",
    "ml_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dbb6d7",
   "metadata": {},
   "source": [
    "## COMBI cabin + ticket + family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e4fcad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean   : 87.5%\n",
      "median : 87.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "divide by zero encountered in true_divide\n",
      "divide by zero encountered in log\n",
      "categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sex': 1.4122783145284568, 'louvain_cabin': 0.3735955069637859, 'wcc_cabin': 0.1735441600962264, 'Age': 0.14968979718908246, 'louvain_ticket': 0.14604380173480347, 'fastrp_ticket2': 0.1421121871105653, 'node2vec_cabin2': 0.12964586469303832, 'node2vec_family1': 0.1256783678489946, 'node2vec_ticket2': 0.11906966929822344, 'node2vec_cabin4': 0.11065421536942242, 'TicketPrice': 0.10504840677052411, 'Embarked': 0.1012290724433035, 'node2vec_ticket3': 0.09847012896848022, 'degree_ticket': 0.09662193738120256, 'node2vec_family4': 0.08673261212990731, 'node2vec_ticket1': 0.08613459870081709, 'node2vec_family2': 0.07974999138300334, 'pagerank_ticket': 0.07660499804264821, 'node2vec_ticket4': 0.06925698688001902, 'pagerank_cabin': 0.0663027208857047, 'node2vec_cabin1': 0.0659701976101762, 'louvain_family': 0.06514675415874896, 'node2vec_cabin3': 0.06179031282202072, 'node2vec_family3': 0.06140173155578417, 'trianglesCount_family': 0.05885812171309304, 'wcc_family': 0.057366055013599856, 'fastrp_cabin2': 0.04953201837514529, 'degree_family': 0.04339903313226643, 'wcc_ticket': 0.03913125150929369, 'fastrp_family3': 0.03684925113422284, 'fastrp_family1': 0.034693381630904206, 'NrSiblings': 0.033580993786454925, 'fastrp_family4': 0.025175260387893122, 'fastrp_ticket4': 0.023734729943232176, 'fastrp_family2': 0.023117716824550286, 'fastrp_cabin3': 0.018149396681113858, 'fastrp_ticket1': 0.016929962464567242, 'fastrp_ticket3': 0.016748599965172237, 'clusteringCoefficient_family': 0.014049189844520277, 'trianglesCount_ticket': 0.012181850331404629, 'pagerank_family': 0.01092128517461745, 'fastrp_cabin4': 0.006552064230892495, 'Pclass': 0.0026076643236864474, 'degree_cabin': 0.002194768561461279, 'betweenness_family': 0.0017640824609592596, 'NrParents': 0.001693255862126342, 'fastrp_cabin1': 0.001630637048576166, 'trianglesCount_cabin': 0.0, 'clusteringCoefficient_cabin': 0.0, 'clusteringCoefficient_ticket': 0.0, 'betweenness_ticket': 0.0, 'betweenness_cabin': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "MATCH (p:Passenger)\n",
    "RETURN p.sex AS Sex,\n",
    "       p.age AS Age,\n",
    "       p.nrSiblings AS NrSiblings,\n",
    "       p.nrParents AS NrParents,\n",
    "       p.ticketPrice AS TicketPrice,\n",
    "       p.survived AS Survived,\n",
    "       p.pclass AS Pclass,\n",
    "       p.port AS Embarked,\n",
    "       \n",
    "       //p.cabinId AS Cabin,\n",
    "       //p.ticketId AS TicketId,\n",
    "       //p.passengerId AS PassengerId,\n",
    "       \n",
    "       p.trianglesCount_cabin AS trianglesCount_cabin,\n",
    "       p.clusteringCoefficient_cabin AS clusteringCoefficient_cabin,\n",
    "       p.betweenness_cabin AS betweenness_cabin,\n",
    "       p.degree_cabin AS degree_cabin,\n",
    "       p.weaklyconnectedcomponents_cabin AS wcc_cabin,\n",
    "       p.louvain_cabin AS louvain_cabin,\n",
    "       p.pagerank_cabin AS pagerank_cabin,\n",
    "       p.node2vec_cabin AS node2vec_cabin,\n",
    "       p.fastrp_cabin AS fastrp_cabin,\n",
    "       \n",
    "       p.trianglesCount_ticket AS trianglesCount_ticket,\n",
    "       p.clusteringCoefficient_ticket AS clusteringCoefficient_ticket,\n",
    "       p.betweenness_ticket AS betweenness_ticket,\n",
    "       p.degree_ticket AS degree_ticket,\n",
    "       p.weaklyconnectedcomponents_ticket AS wcc_ticket,\n",
    "       p.louvain_ticket AS louvain_ticket,\n",
    "       p.pagerank_ticket AS pagerank_ticket,\n",
    "       p.node2vec_ticket AS node2vec_ticket,\n",
    "       p.fastrp_ticket AS fastrp_ticket,\n",
    "       \n",
    "       p.trianglesCount_family AS trianglesCount_family,\n",
    "       p.clusteringCoefficient_family AS clusteringCoefficient_family,\n",
    "       p.betweenness_family AS betweenness_family,\n",
    "       p.degree_family AS degree_family,\n",
    "       p.weaklyconnectedcomponents_family AS wcc_family,\n",
    "       p.louvain_family AS louvain_family,\n",
    "       p.pagerank_family AS pagerank_family,\n",
    "       p.node2vec_family AS node2vec_family,\n",
    "       p.fastrp_family AS fastrp_family\n",
    "\"\"\"\n",
    "df = graph.run(query).to_data_frame()\n",
    "df = df.astype({'Survived':'int', 'Sex':'str', 'Embarked':'str', 'Pclass':'str'})\n",
    "# df = df.astype({'Cabin':'str'})\n",
    "# df = df.astype({'TicketId':'int'})\n",
    "# df = df.astype({'PassengerId':'int'})\n",
    "df = df.astype({'trianglesCount_cabin':'int',\n",
    "                'clusteringCoefficient_cabin':'object',\n",
    "                'betweenness_cabin':'object',\n",
    "                'degree_cabin':'int',\n",
    "                'wcc_cabin':'int',\n",
    "                'pagerank_cabin':'float',\n",
    "                'louvain_cabin':'int'})\n",
    "df = df.astype({'trianglesCount_ticket':'int',\n",
    "                'clusteringCoefficient_ticket':'object',\n",
    "                'betweenness_ticket':'object',\n",
    "                'degree_ticket':'int',\n",
    "                'wcc_ticket':'int',\n",
    "                'pagerank_ticket':'float',\n",
    "                'louvain_ticket':'int'})\n",
    "df = df.astype({'trianglesCount_family':'int',\n",
    "                'clusteringCoefficient_family':'object',\n",
    "                'betweenness_family':'object',\n",
    "                'degree_family':'int',\n",
    "                'wcc_family':'int',\n",
    "                'pagerank_family':'float',\n",
    "                'louvain_family':'int'})\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['node2vec_cabin'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('node2vec_cabin'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['node2vec_cabin'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['node2vec_cabin'], axis=1)\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['fastrp_cabin'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('fastrp_cabin'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['fastrp_cabin'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['fastrp_cabin'], axis=1)\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['node2vec_ticket'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('node2vec_ticket'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['node2vec_ticket'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['node2vec_ticket'], axis=1)\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['fastrp_ticket'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('fastrp_ticket'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['fastrp_ticket'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['fastrp_ticket'], axis=1)\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['node2vec_family'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('node2vec_family'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['node2vec_family'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['node2vec_family'], axis=1)\n",
    "\n",
    "# embedding dimension\n",
    "length = len(df['fastrp_family'][0])\n",
    "# create features\n",
    "columns = []\n",
    "for i in range(length):\n",
    "    columns.append('fastrp_family'+str(i+1))\n",
    "# transform embedding into dataframe columns\n",
    "embedding_df = pd.DataFrame(df['fastrp_family'].tolist(), columns=columns)\n",
    "# add embedding features to larger dataframe\n",
    "df = pd.concat([df, embedding_df], axis=1)\n",
    "df = df.drop(['fastrp_family'], axis=1)\n",
    "\n",
    "ml_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35426b6f",
   "metadata": {},
   "source": [
    "nothing      = 84.5%  \n",
    "  \n",
    "cabin        = 84.8%  \n",
    "cabin graph  = 85.4%  \n",
    "  \n",
    "ticket       = 85.9%  \n",
    "ticket graph = 87.8%  \n",
    "  \n",
    "passenger    = 85.2%  \n",
    "  \n",
    "family graph = 86.8%  \n",
    "  \n",
    "combi        = 87.5%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c65b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_neo4j",
   "language": "python",
   "name": "conda-env-py38_neo4j-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
